name: Quality Assurance Pipeline

on:
  push:
    branches: [ master, main, develop, staging ]
  pull_request:
    branches: [ master, main, develop ]
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.10'
  NODE_VERSION: '18'

jobs:
  code-quality:
    name: Code Quality Analysis
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-quality-${{ hashFiles('app/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-quality-
          ${{ runner.os }}-pip-
    
    - name: Install quality tools
      run: |
        python -m pip install --upgrade pip
        cd app
        pip install -r requirements.txt
        pip install pylint flake8 black isort mypy radon
        pip install pytest pytest-cov pytest-html pytest-benchmark
    
    - name: Code formatting check (Black)
      continue-on-error: true
      run: |
        echo "ðŸŽ¨ ì½”ë“œ í¬ë§·íŒ… í™•ì¸..."
        cd app
        black --check --diff . || echo "âš ï¸ ì½”ë“œ í¬ë§·íŒ… ê°œì„ ì‚¬í•­ ë°œê²¬"
    
    - name: Import sorting check (isort)
      continue-on-error: true
      run: |
        echo "ðŸ“¦ Import ì •ë ¬ í™•ì¸..."
        cd app
        isort --check-only --diff . || echo "âš ï¸ Import ì •ë ¬ ê°œì„ ì‚¬í•­ ë°œê²¬"
    
    - name: Linting (flake8)
      continue-on-error: true
      run: |
        echo "ðŸ” ì½”ë“œ ë¦°íŒ…..."
        cd app
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics --exit-zero
        flake8 . --count --exit-zero --max-complexity=15 --max-line-length=150 --statistics --output-file=flake8-report.txt
    
    - name: Advanced linting (pylint)
      run: |
        echo "ðŸ”Ž ê³ ê¸‰ ì½”ë“œ ë¶„ì„..."
        cd app
        pylint --output-format=text --reports=yes --score=yes $(find . -name "*.py" | head -10) > pylint-report.txt || echo "Pylint warnings found"
    
    - name: Type checking (mypy)
      continue-on-error: true
      run: |
        echo "ðŸ·ï¸ íƒ€ìž… ì²´í‚¹..."
        cd app
        mypy --ignore-missing-imports --no-strict-optional . > mypy-report.txt || echo "Type checking issues found"
    
    - name: Code complexity analysis
      run: |
        echo "ðŸ“Š ì½”ë“œ ë³µìž¡ë„ ë¶„ì„..."
        cd app
        radon cc . --show-complexity --min C > complexity-report.txt || echo "No complex code found"
        radon mi . --show --min B >> complexity-report.txt || echo "Maintainability index calculated"
    
    - name: Documentation coverage
      run: |
        echo "ðŸ“š ë¬¸ì„œí™” ì»¤ë²„ë¦¬ì§€ í™•ì¸..."
        cd app
        python ../scripts/doc_coverage.py || echo "Documentation analysis failed"
    
    - name: Upload quality reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: quality-reports-${{ github.run_id }}
        path: |
          app/flake8-report.txt
          app/pylint-report.txt
          app/mypy-report.txt
          app/complexity-report.txt
          app/doc-coverage.txt
        retention-days: 30
    
    - name: Quality gates check
      continue-on-error: true
      run: |
        cd app
        echo "ðŸšª í’ˆì§ˆ ê²Œì´íŠ¸ í™•ì¸..."
        
        # ë³µìž¡ë„ ì²´í¬
        COMPLEX_FILES=$(radon cc . --min C --show-complexity | wc -l || echo "0")
        if [ "${COMPLEX_FILES:-0}" -gt 10 ]; then
          echo "âš ï¸ Many complex functions: $COMPLEX_FILES"
        else
          echo "âœ… ë³µìž¡ë„ ì²´í¬ í†µê³¼"
        fi
        
        # ë¬¸ì„œí™” ì»¤ë²„ë¦¬ì§€ ì²´í¬ (bc ì—†ì´ ì²˜ë¦¬)
        if [ -f "doc-coverage.txt" ]; then
          DOC_COVERAGE=$(cat doc-coverage.txt)
          echo "ðŸ“š ë¬¸ì„œí™” ì»¤ë²„ë¦¬ì§€: ${DOC_COVERAGE}%"
        else
          echo "ðŸ“š ë¬¸ì„œí™” ì»¤ë²„ë¦¬ì§€: ì²´í¬ ë¶ˆê°€"
        fi
        
        echo "âœ… í’ˆì§ˆ ê²Œì´íŠ¸ ì™„ë£Œ!"

  performance-test:
    name: Performance Testing
    runs-on: ubuntu-latest
    continue-on-error: true
    
    services:
      mysql:
        image: mysql:8.0
        env:
          MYSQL_ROOT_PASSWORD: test123
          MYSQL_DATABASE: safework_test
        ports:
          - 3306:3306
        options: >-
          --health-cmd="mysqladmin ping"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=3
      
      redis:
        image: redis:alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd="redis-cli ping"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=3
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        cd app
        pip install -r requirements.txt
        pip install pytest-benchmark locust requests-mock
    
    - name: Database performance test
      run: |
        echo "ðŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸..."
        cd app
        python -c "
import time
import pymysql
from sqlalchemy import create_engine, text

# DB ì—°ê²° í…ŒìŠ¤íŠ¸
engine = create_engine('mysql+pymysql://root:test123@localhost:3306/safework_test')

# ê°„ë‹¨í•œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
start_time = time.time()
with engine.connect() as conn:
    for i in range(100):
        result = conn.execute(text('SELECT 1'))
connection_time = time.time() - start_time

print(f'Database connection performance: {connection_time:.3f}s for 100 queries')
if connection_time > 5.0:
    print('âš ï¸ Database performance warning')
else:
    print('âœ… Database performance OK')
"
    
    - name: Application benchmark tests
      run: |
        echo "ðŸš€ ì• í”Œë¦¬ì¼€ì´ì…˜ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸..."
        cd app
        
        # í•µì‹¬ í•¨ìˆ˜ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        python -c "
import sys
import time
from models import Survey

# ë”ë¯¸ ë°ì´í„°ë¡œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
def benchmark_survey_creation():
    start = time.time()
    survey_data = {
        'name': 'Test User',
        'age': 30,
        'department': 'IT'
    }
    # Survey ê°ì²´ ìƒì„± ì‹œê°„ ì¸¡ì •
    for _ in range(1000):
        survey = Survey(**survey_data)
    end = time.time()
    return end - start

exec_time = benchmark_survey_creation()
print(f'Survey creation benchmark: {exec_time:.3f}s for 1000 objects')
if exec_time > 1.0:
    print('âš ï¸ Performance degradation detected')
    sys.exit(1)
else:
    print('âœ… Performance benchmark passed')
"

  load-test:
    name: Load Testing
    runs-on: ubuntu-latest
    continue-on-error: true
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/staging' || github.ref == 'refs/heads/master'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install load testing tools
      run: |
        pip install locust requests faker
    
    - name: Create load test script
      run: |
        cat > load_test.py << 'EOF'
from locust import HttpUser, task, between
import json
from faker import Faker

fake = Faker()

class SafeworkUser(HttpUser):
    wait_time = between(1, 3)
    
    def on_start(self):
        """í…ŒìŠ¤íŠ¸ ì‹œìž‘ ì‹œ ì‹¤í–‰"""
        # ê±´ê°• ìƒíƒœ í™•ì¸
        self.client.get("/health")
    
    @task(3)
    def view_homepage(self):
        """í™ˆíŽ˜ì´ì§€ ì¡°íšŒ"""
        self.client.get("/")
    
    @task(2)
    def view_survey_form(self):
        """ì„¤ë¬¸ì¡°ì‚¬ í¼ ì¡°íšŒ"""
        self.client.get("/survey/new")
    
    @task(1)
    def health_check(self):
        """í—¬ìŠ¤ì²´í¬"""
        with self.client.get("/health", catch_response=True) as response:
            if response.status_code == 200:
                response.success()
            else:
                response.failure(f"Health check failed: {response.status_code}")
    
    @task(1)
    def static_resources(self):
        """ì •ì  ë¦¬ì†ŒìŠ¤ ë¡œë“œ"""
        self.client.get("/static/css/style.css", name="/static/css")
        self.client.get("/static/js/app.js", name="/static/js")
EOF
    
    - name: Run load test (smoke test)
      run: |
        echo "ðŸ”¥ ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ìŠ¤ëª¨í¬ í…ŒìŠ¤íŠ¸)..."
        # ì§§ì€ ë¶€í•˜ í…ŒìŠ¤íŠ¸ (CIìš©)
        locust -f load_test.py --headless --users 10 --spawn-rate 2 --run-time 30s --host http://localhost:4545 --html load-test-report.html || echo "Load test completed with warnings"
    
    - name: Upload load test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: load-test-report-${{ github.run_id }}
        path: load-test-report.html
        retention-days: 30

  accessibility-test:
    name: Accessibility Testing
    runs-on: ubuntu-latest
    continue-on-error: true
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
    
    - name: Install accessibility tools
      run: |
        npm install -g axe-cli pa11y lighthouse-ci
    
    - name: Set up Python and start app
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Start application for testing
      run: |
        cd app
        pip install -r requirements.txt
        # í…ŒìŠ¤íŠ¸ìš© ê°„ë‹¨í•œ ì„œë²„ ì‹œìž‘
        python -c "
from app import create_app
app = create_app('testing')
app.run(host='0.0.0.0', port=5000, debug=False)
" &
        
        # ì„œë²„ ì‹œìž‘ ëŒ€ê¸°
        sleep 10
    
    - name: Accessibility audit with axe
      continue-on-error: true
      run: |
        echo "â™¿ ì ‘ê·¼ì„± í…ŒìŠ¤íŠ¸..."
        axe http://localhost:5000 --save axe-results.json --reporter json
        axe http://localhost:5000/survey/new --save axe-survey-results.json --reporter json
    
    - name: Accessibility audit with pa11y
      continue-on-error: true
      run: |
        echo "â™¿ Pa11y ì ‘ê·¼ì„± í…ŒìŠ¤íŠ¸..."
        pa11y http://localhost:5000 --reporter json > pa11y-results.json || echo "Accessibility issues found"
        pa11y http://localhost:5000/survey/new --reporter json > pa11y-survey-results.json || echo "Survey accessibility issues found"
    
    - name: Lighthouse CI audit
      continue-on-error: true
      run: |
        echo "ðŸ’¡ Lighthouse ì„±ëŠ¥ ë° ì ‘ê·¼ì„± í…ŒìŠ¤íŠ¸..."
        lhci autorun --upload.target=temporary-public-storage --collect.url=http://localhost:5000
    
    - name: Upload accessibility reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: accessibility-reports-${{ github.run_id }}
        path: |
          axe-results.json
          axe-survey-results.json
          pa11y-results.json
          pa11y-survey-results.json
        retention-days: 30

  quality-summary:
    name: Quality Summary
    runs-on: ubuntu-latest
    needs: [code-quality, performance-test, accessibility-test]
    if: always()
    
    steps:
    - name: Download quality reports
      uses: actions/download-artifact@v4
      with:
        name: quality-reports-${{ github.run_id }}
        path: ./reports
      continue-on-error: true
    
    - name: Generate quality summary
      run: |
        echo "## ðŸ† Quality Assessment Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        echo "### ðŸ“Š Code Quality" >> $GITHUB_STEP_SUMMARY
        if [ -f reports/doc-coverage.txt ]; then
          DOC_COV=$(cat reports/doc-coverage.txt)
          echo "- **Documentation Coverage**: ${DOC_COV}%" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "- **Code Quality Check**: ${{ needs.code-quality.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Performance Test**: ${{ needs.performance-test.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Accessibility Test**: ${{ needs.accessibility-test.result }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ needs.code-quality.result }}" = "success" ] && [ "${{ needs.performance-test.result }}" = "success" ]; then
          echo "âœ… **ì „ì²´ í’ˆì§ˆ í‰ê°€: PASSED**" >> $GITHUB_STEP_SUMMARY
        else
          echo "âŒ **ì „ì²´ í’ˆì§ˆ í‰ê°€: NEEDS ATTENTION**" >> $GITHUB_STEP_SUMMARY
        fi