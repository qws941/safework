#!/usr/bin/env python3
"""
SafeWork GitHub ì´ìŠˆ ëŒ“ê¸€ ì •ë¦¬ ìë™í™” ì‹œìŠ¤í…œ
í•´ì†Œëœ ì´ìŠˆì˜ ë¶ˆí•„ìš”í•œ ëŒ“ê¸€ì„ ì •ë¦¬í•˜ê³  ìµœì¢… ìƒíƒœë¥¼ ëª…í™•í•˜ê²Œ ìœ ì§€
"""

import os
import json
import requests
import time
import re
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Optional, Tuple, Set
import argparse
import sys
from dataclasses import dataclass

@dataclass
class CommentAnalysis:
    id: int
    body: str
    author: str
    created_at: datetime
    is_bot: bool
    is_spam: bool
    is_duplicate: bool
    is_outdated: bool
    is_noise: bool
    importance_score: int
    cleanup_reason: str
    should_keep: bool

class SafeWorkCommentCleanup:
    def __init__(self, github_token: str, repository: str):
        """
        SafeWork ëŒ“ê¸€ ì •ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            github_token: GitHub Personal Access Token  
            repository: Repository name (format: owner/repo)
        """
        self.github_token = github_token
        self.repository = repository
        self.api_base = "https://api.github.com"
        self.headers = {
            "Authorization": f"token {github_token}",
            "Accept": "application/vnd.github.v3+json",
            "User-Agent": "SafeWork-Comment-Cleanup-System/1.0"
        }
        
        # ì •ë¦¬ ëŒ€ìƒ í‚¤ì›Œë“œë“¤
        self.cleanup_keywords = [
            "ì²˜ë¦¬ ì¤‘", "ì§„í–‰ ì¤‘", "ì„ì‹œ", "test", "í…ŒìŠ¤íŠ¸", "debugging", 
            "ì‘ì—…ì¤‘", "WIP", "work in progress", "ì²´í¬", "í™•ì¸ì¤‘",
            "ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤", "ê²€í† ì¤‘", "ë¶„ì„ì¤‘"
        ]
        
        # ì¤‘ìš”í•œ í‚¤ì›Œë“œë“¤ (ë³´ì¡´í•´ì•¼ í•  ëŒ“ê¸€)
        self.important_keywords = [
            "í•´ê²°", "ì™„ë£Œ", "fixed", "resolved", "merged", "deployed",
            "ê²€ì¦", "ìŠ¹ì¸", "í™•ì¸ë¨", "í…ŒìŠ¤íŠ¸ í†µê³¼", "ì¦ëª…", "ìµœì¢…"
        ]
        
        # ìŠ¤íŒ¸/ë…¸ì´ì¦ˆ íŒ¨í„´
        self.spam_patterns = [
            r'^[\.\!\?]+$',  # ì ì´ë‚˜ ëŠë‚Œí‘œë§Œ
            r'^[ã…‹ã…]+$',    # ã…‹ã…‹, ã…ã…ë§Œ
            r'^\+1$|^-1$',   # +1, -1ë§Œ
            r'^ğŸ‘$|^ğŸ‘$',    # ì´ëª¨ì§€ë§Œ
            r'^(ê°ì‚¬|ê³ ìƒ|ìˆ˜ê³ ).*$'  # ë‹¨ìˆœ ê°ì‚¬ ì¸ì‚¬
        ]
        
        self.session = requests.Session()
        self.session.headers.update(self.headers)

    def get_issue_comments(self, issue_number: int) -> List[Dict]:
        """ì´ìŠˆì˜ ëª¨ë“  ëŒ“ê¸€ ì¡°íšŒ"""
        print(f"ğŸ’¬ ì´ìŠˆ #{issue_number} ëŒ“ê¸€ ëª©ë¡ ì¡°íšŒ ì¤‘...")
        
        url = f"{self.api_base}/repos/{self.repository}/issues/{issue_number}/comments"
        comments = []
        page = 1
        
        while True:
            response = self.session.get(url, params={'page': page, 'per_page': 100})
            
            if response.status_code != 200:
                raise Exception(f"ëŒ“ê¸€ ì¡°íšŒ ì‹¤íŒ¨ (HTTP {response.status_code}): {response.text}")
            
            page_comments = response.json()
            if not page_comments:
                break
                
            comments.extend(page_comments)
            page += 1
        
        print(f"âœ… ì´ {len(comments)}ê°œ ëŒ“ê¸€ ì¡°íšŒ ì™„ë£Œ")
        return comments

    def analyze_comment(self, comment_data: Dict, all_comments: List[Dict]) -> CommentAnalysis:
        """ê°œë³„ ëŒ“ê¸€ ë¶„ì„"""
        body = comment_data['body'].strip()
        author = comment_data['user']['login']
        created_at = datetime.fromisoformat(comment_data['created_at'].replace('Z', '+00:00'))
        comment_id = comment_data['id']
        
        # ê¸°ë³¸ ë¶„ì„
        is_bot = comment_data['user']['type'] == 'Bot' or author.endswith('[bot]')
        
        # ìŠ¤íŒ¸/ë…¸ì´ì¦ˆ ê²€ì‚¬
        is_spam = self._is_spam_comment(body)
        is_noise = self._is_noise_comment(body)
        
        # ì¤‘ë³µ ê²€ì‚¬
        is_duplicate = self._is_duplicate_comment(comment_data, all_comments)
        
        # ì‹œíš¨ì„± ê²€ì‚¬ (30ì¼ ì´ìƒ ëœ ì„ì‹œ ëŒ“ê¸€)
        is_outdated = self._is_outdated_comment(body, created_at)
        
        # ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚° (1-10)
        importance_score = self._calculate_importance_score(body, author, is_bot, created_at)
        
        # ì •ë¦¬ ì‚¬ìœ  ê²°ì •
        cleanup_reason = self._determine_cleanup_reason(
            is_spam, is_noise, is_duplicate, is_outdated, importance_score
        )
        
        # ë³´ì¡´ ì—¬ë¶€ ê²°ì •
        should_keep = self._should_keep_comment(
            body, importance_score, is_spam, is_noise, is_duplicate, is_outdated
        )
        
        return CommentAnalysis(
            id=comment_id,
            body=body,
            author=author,
            created_at=created_at,
            is_bot=is_bot,
            is_spam=is_spam,
            is_duplicate=is_duplicate,
            is_outdated=is_outdated,
            is_noise=is_noise,
            importance_score=importance_score,
            cleanup_reason=cleanup_reason,
            should_keep=should_keep
        )

    def _is_spam_comment(self, body: str) -> bool:
        """ìŠ¤íŒ¸ ëŒ“ê¸€ íŒë‹¨"""
        body_lower = body.lower()
        
        # íŒ¨í„´ ê¸°ë°˜ ê²€ì‚¬
        for pattern in self.spam_patterns:
            if re.match(pattern, body.strip()):
                return True
        
        # ê¸¸ì´ ê¸°ë°˜ ê²€ì‚¬
        if len(body.strip()) < 3:
            return True
        
        # ë°˜ë³µ ë¬¸ì ê²€ì‚¬
        if re.match(r'^(.)\1{5,}$', body.strip()):  # ê°™ì€ ë¬¸ì 6ë²ˆ ì´ìƒ
            return True
            
        return False

    def _is_noise_comment(self, body: str) -> bool:
        """ë…¸ì´ì¦ˆ ëŒ“ê¸€ íŒë‹¨"""
        body_lower = body.lower()
        
        # ì •ë¦¬ ëŒ€ìƒ í‚¤ì›Œë“œ í¬í•¨
        for keyword in self.cleanup_keywords:
            if keyword in body_lower:
                return True
        
        # ë´‡ ìë™ ëŒ“ê¸€ íŒ¨í„´
        bot_patterns = [
            "ìë™ìœ¼ë¡œ ìƒì„±ëœ", "automatically generated", 
            "bot comment", "automated message"
        ]
        
        for pattern in bot_patterns:
            if pattern in body_lower:
                return True
                
        return False

    def _is_duplicate_comment(self, comment_data: Dict, all_comments: List[Dict]) -> bool:
        """ì¤‘ë³µ ëŒ“ê¸€ íŒë‹¨"""
        current_body = comment_data['body'].strip().lower()
        current_id = comment_data['id']
        current_created = comment_data['created_at']
        
        # ìœ ì‚¬í•œ ëŒ“ê¸€ ì°¾ê¸°
        for other_comment in all_comments:
            if other_comment['id'] == current_id:
                continue
                
            other_body = other_comment['body'].strip().lower()
            
            # ì™„ì „ ì¼ì¹˜
            if current_body == other_body:
                # ë” ì˜¤ë˜ëœ ëŒ“ê¸€ì´ë©´ í˜„ì¬ ëŒ“ê¸€ì´ ì¤‘ë³µ
                if current_created > other_comment['created_at']:
                    return True
            
            # 90% ì´ìƒ ìœ ì‚¬
            if self._similarity_ratio(current_body, other_body) > 0.9:
                if current_created > other_comment['created_at']:
                    return True
        
        return False

    def _is_outdated_comment(self, body: str, created_at: datetime) -> bool:
        """ì‹œíš¨ê°€ ì§€ë‚œ ëŒ“ê¸€ íŒë‹¨"""
        # 30ì¼ ì´ìƒ ëœ ì„ì‹œ ëŒ“ê¸€
        thirty_days_ago = datetime.now(timezone.utc) - timedelta(days=30)
        
        if created_at < thirty_days_ago:
            body_lower = body.lower()
            temporary_keywords = ["ì„ì‹œ", "temporary", "wip", "ì‘ì—…ì¤‘", "ì§„í–‰ì¤‘"]
            
            for keyword in temporary_keywords:
                if keyword in body_lower:
                    return True
        
        return False

    def _calculate_importance_score(self, body: str, author: str, 
                                   is_bot: bool, created_at: datetime) -> int:
        """ëŒ“ê¸€ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚° (1-10)"""
        score = 5  # ê¸°ë³¸ ì ìˆ˜
        body_lower = body.lower()
        
        # ì¤‘ìš”í•œ í‚¤ì›Œë“œ í¬í•¨ì‹œ ì ìˆ˜ ì¦ê°€
        for keyword in self.important_keywords:
            if keyword in body_lower:
                score += 2
                break
        
        # ê¸¸ì´ì— ë”°ë¥¸ ì ìˆ˜ ì¡°ì •
        if len(body) > 100:
            score += 1
        elif len(body) < 20:
            score -= 1
        
        # ì‘ì„±ìì— ë”°ë¥¸ ì ìˆ˜ ì¡°ì •
        if author in ['qws941', 'seonmin994']:  # í”„ë¡œì íŠ¸ ë¦¬ë”ë“¤
            score += 1
        elif is_bot:
            score -= 2
        
        # ìµœì‹ ì„±ì— ë”°ë¥¸ ì ìˆ˜ ì¡°ì •
        recent_time = datetime.now(timezone.utc) - timedelta(days=7)
        if created_at > recent_time:
            score += 1
        
        # ìŠ¤ë ˆë“œì˜ ì²« ëŒ“ê¸€ì´ë©´ ì¤‘ìš”ë„ ì¦ê°€
        if "ì²« ëŒ“ê¸€" in body_lower or "first comment" in body_lower:
            score += 1
        
        return max(1, min(10, score))  # 1-10 ë²”ìœ„ ì œí•œ

    def _determine_cleanup_reason(self, is_spam: bool, is_noise: bool, 
                                 is_duplicate: bool, is_outdated: bool, 
                                 importance_score: int) -> str:
        """ì •ë¦¬ ì‚¬ìœ  ê²°ì •"""
        reasons = []
        
        if is_spam:
            reasons.append("ìŠ¤íŒ¸/ì˜ë¯¸ì—†ëŠ” ëŒ“ê¸€")
        if is_noise:
            reasons.append("ë…¸ì´ì¦ˆ/ì„ì‹œ ëŒ“ê¸€")
        if is_duplicate:
            reasons.append("ì¤‘ë³µ ëŒ“ê¸€")
        if is_outdated:
            reasons.append("ì‹œíš¨ ë§Œë£Œ")
        if importance_score <= 2:
            reasons.append("ë‚®ì€ ì¤‘ìš”ë„")
        
        return ", ".join(reasons) if reasons else "ì •ë¦¬ ë¶ˆí•„ìš”"

    def _should_keep_comment(self, body: str, importance_score: int,
                           is_spam: bool, is_noise: bool, 
                           is_duplicate: bool, is_outdated: bool) -> bool:
        """ëŒ“ê¸€ ë³´ì¡´ ì—¬ë¶€ ê²°ì •"""
        # ì¤‘ìš”í•œ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ë¬´ì¡°ê±´ ë³´ì¡´
        body_lower = body.lower()
        for keyword in self.important_keywords:
            if keyword in body_lower:
                return True
        
        # ìŠ¤íŒ¸ì´ë©´ ì œê±°
        if is_spam:
            return False
        
        # ì¤‘ìš”ë„ê°€ ë†’ìœ¼ë©´ ë³´ì¡´
        if importance_score >= 7:
            return True
        
        # ë³µí•©ì  íŒë‹¨
        negative_factors = sum([is_noise, is_duplicate, is_outdated])
        
        # ë¶€ì •ì  ìš”ì†Œê°€ 2ê°œ ì´ìƒì´ê³  ì¤‘ìš”ë„ê°€ ë‚®ìœ¼ë©´ ì œê±°
        if negative_factors >= 2 and importance_score <= 4:
            return False
        
        # ê¸°ë³¸ì ìœ¼ë¡œ ë³´ì¡´
        return True

    def _similarity_ratio(self, text1: str, text2: str) -> float:
        """ë‘ í…ìŠ¤íŠ¸ì˜ ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ êµ¬í˜„)"""
        if not text1 or not text2:
            return 0.0
        
        # ë‹¨ì–´ ê¸°ë°˜ ìœ ì‚¬ë„
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        
        return intersection / union if union > 0 else 0.0

    def minimize_comment(self, comment_id: int, original_body: str, reason: str) -> Dict:
        """ëŒ“ê¸€ ìµœì†Œí™” (ì‹¤ì œ ì‚­ì œ ëŒ€ì‹  ë‚´ìš© ìµœì†Œí™”)"""
        print(f"ğŸ”„ ëŒ“ê¸€ {comment_id} ìµœì†Œí™” ì¤‘...")
        
        minimized_body = f"""~~[ìë™ ì •ë¦¬ë¨: {reason}]~~

<details>
<summary>ì›ë³¸ ëŒ“ê¸€ ë³´ê¸°</summary>

{original_body[:200]}{'...' if len(original_body) > 200 else ''}

</details>

---
*SafeWork Comment Cleanup Systemì— ì˜í•´ ìë™ ì •ë¦¬ë¨*"""

        url = f"{self.api_base}/repos/{self.repository}/issues/comments/{comment_id}"
        data = {"body": minimized_body}
        
        response = self.session.patch(url, json=data)
        
        if response.status_code == 200:
            print(f"âœ… ëŒ“ê¸€ {comment_id} ìµœì†Œí™” ì™„ë£Œ")
            return response.json()
        else:
            print(f"âŒ ëŒ“ê¸€ {comment_id} ìµœì†Œí™” ì‹¤íŒ¨: {response.status_code}")
            return {}

    def create_cleanup_summary(self, issue_number: int, analyses: List[CommentAnalysis], 
                              cleaned_count: int) -> Dict:
        """ì •ë¦¬ ìš”ì•½ ëŒ“ê¸€ ìƒì„±"""
        print(f"ğŸ“‹ ì´ìŠˆ #{issue_number} ì •ë¦¬ ìš”ì•½ ìƒì„± ì¤‘...")
        
        total_comments = len(analyses)
        kept_comments = len([a for a in analyses if a.should_keep])
        
        # ì •ë¦¬ ì´ìœ ë³„ í†µê³„
        cleanup_reasons = {}
        for analysis in analyses:
            if not analysis.should_keep:
                reason = analysis.cleanup_reason
                cleanup_reasons[reason] = cleanup_reasons.get(reason, 0) + 1
        
        # ì¤‘ìš”ë„ë³„ í†µê³„
        importance_stats = {
            "high": len([a for a in analyses if a.importance_score >= 7]),
            "medium": len([a for a in analyses if 4 <= a.importance_score < 7]), 
            "low": len([a for a in analyses if a.importance_score < 4])
        }
        
        summary_comment = f"""ğŸ§¹ **ì´ìŠˆ ëŒ“ê¸€ ìë™ ì •ë¦¬ ì™„ë£Œ**

## ğŸ“Š ì •ë¦¬ í†µê³„
- **ì „ì²´ ëŒ“ê¸€**: {total_comments}ê°œ
- **ë³´ì¡´ëœ ëŒ“ê¸€**: {kept_comments}ê°œ  
- **ì •ë¦¬ëœ ëŒ“ê¸€**: {cleaned_count}ê°œ
- **ì •ë¦¬ ì™„ë£Œ ì‹œê°„**: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')}

## ğŸ” ì •ë¦¬ ê¸°ì¤€ ì ìš© ê²°ê³¼"""

        # ì •ë¦¬ ì´ìœ ë³„ í†µê³„ ì¶”ê°€
        if cleanup_reasons:
            summary_comment += "\n\n### ğŸ“ ì •ë¦¬ ì´ìœ ë³„ í†µê³„"
            for reason, count in cleanup_reasons.items():
                summary_comment += f"\n- **{reason}**: {count}ê°œ"
        
        # ì¤‘ìš”ë„ë³„ í†µê³„ ì¶”ê°€  
        summary_comment += f"""

### â­ ì¤‘ìš”ë„ë³„ ë¶„ì„
- **ë†’ìŒ (7-10ì )**: {importance_stats['high']}ê°œ - ëª¨ë‘ ë³´ì¡´
- **ë³´í†µ (4-6ì )**: {importance_stats['medium']}ê°œ - ì¡°ê±´ë¶€ ë³´ì¡´
- **ë‚®ìŒ (1-3ì )**: {importance_stats['low']}ê°œ - ëŒ€ë¶€ë¶„ ì •ë¦¬

## ğŸ¯ ì •ë¦¬ ì •ì±…
1. **ì¤‘ìš”í•œ í‚¤ì›Œë“œ** í¬í•¨ ëŒ“ê¸€ì€ ë¬´ì¡°ê±´ ë³´ì¡´
   - í•´ê²°, ì™„ë£Œ, fixed, resolved, ê²€ì¦, ìŠ¹ì¸ ë“±
2. **ìŠ¤íŒ¸/ë…¸ì´ì¦ˆ** ëŒ“ê¸€ ìë™ ì •ë¦¬
   - ì˜ë¯¸ì—†ëŠ” ë¬¸ì, ë°˜ë³µ ëŒ“ê¸€, ì„ì‹œ ë©”ëª¨ ë“±
3. **ì¤‘ë³µ ëŒ“ê¸€** ì •ë¦¬ (90% ì´ìƒ ìœ ì‚¬)
4. **ì‹œíš¨ ë§Œë£Œ** ëŒ“ê¸€ ì •ë¦¬ (30ì¼ ì´ìƒ ëœ ì„ì‹œ ëŒ“ê¸€)

## âœ… ìµœì¢… ìƒíƒœ
ì´ì œ ì´ìŠˆì˜ í•µì‹¬ ë‚´ìš©ê³¼ í•´ê²° ê³¼ì •ë§Œ ëª…í™•í•˜ê²Œ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.

---
*SafeWork Comment Cleanup System v1.0ì— ì˜í•´ ìë™ ìƒì„±ë¨*"""

        url = f"{self.api_base}/repos/{self.repository}/issues/{issue_number}/comments"
        data = {"body": summary_comment}
        
        response = self.session.post(url, json=data)
        
        if response.status_code == 201:
            print(f"âœ… ì •ë¦¬ ìš”ì•½ ëŒ“ê¸€ ì¶”ê°€ ì™„ë£Œ")
            return response.json()
        else:
            print(f"âŒ ì •ë¦¬ ìš”ì•½ ëŒ“ê¸€ ì¶”ê°€ ì‹¤íŒ¨: {response.status_code}")
            return {}

    def process_issue_cleanup(self, issue_number: int, dry_run: bool = False,
                             preserve_recent_days: int = 7,
                             min_importance_threshold: int = 3) -> Dict:
        """ì´ìŠˆ ëŒ“ê¸€ ì •ë¦¬ ì „ì²´ í”„ë¡œì„¸ìŠ¤"""
        print(f"ğŸ§¹ ì´ìŠˆ #{issue_number} ëŒ“ê¸€ ì •ë¦¬ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
        print(f"ğŸ” ì‹¤í–‰ ëª¨ë“œ: {'ì‹œë®¬ë ˆì´ì…˜' if dry_run else 'ì‹¤ì œ ì •ë¦¬'}")
        
        try:
            # 1. ëŒ“ê¸€ ì¡°íšŒ
            comments = self.get_issue_comments(issue_number)
            
            if not comments:
                print("ğŸ’¬ ì •ë¦¬í•  ëŒ“ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
                return {"success": True, "message": "No comments to cleanup"}
            
            # 2. ëŒ“ê¸€ ë¶„ì„
            print("ğŸ” ëŒ“ê¸€ ë¶„ì„ ì‹œì‘...")
            analyses = []
            
            for comment_data in comments:
                analysis = self.analyze_comment(comment_data, comments)
                
                # ìµœê·¼ ëŒ“ê¸€ ë³´í˜¸
                recent_threshold = datetime.now(timezone.utc) - timedelta(days=preserve_recent_days)
                if analysis.created_at > recent_threshold:
                    analysis.should_keep = True
                    analysis.cleanup_reason = "ìµœê·¼ ëŒ“ê¸€ (ë³´í˜¸ë¨)"
                
                analyses.append(analysis)
            
            # 3. ì •ë¦¬ ëŒ€ìƒ ì„ ë³„
            cleanup_candidates = [a for a in analyses if not a.should_keep]
            keep_candidates = [a for a in analyses if a.should_keep]
            
            print(f"ğŸ“Š ë¶„ì„ ê²°ê³¼:")
            print(f"  - ì „ì²´ ëŒ“ê¸€: {len(analyses)}ê°œ")
            print(f"  - ë³´ì¡´ ì˜ˆì •: {len(keep_candidates)}ê°œ")
            print(f"  - ì •ë¦¬ ì˜ˆì •: {len(cleanup_candidates)}ê°œ")
            
            if dry_run:
                print("\nğŸ” ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ - ì •ë¦¬ ëŒ€ìƒ ìƒì„¸:")
                for analysis in cleanup_candidates:
                    print(f"  - ID {analysis.id}: {analysis.cleanup_reason}")
                    print(f"    ì‘ì„±ì: {analysis.author}, ì¤‘ìš”ë„: {analysis.importance_score}")
                    print(f"    ë‚´ìš©: {analysis.body[:50]}...")
                    print()
                
                return {
                    "success": True,
                    "dry_run": True,
                    "total_comments": len(analyses),
                    "cleanup_candidates": len(cleanup_candidates),
                    "preservation_candidates": len(keep_candidates)
                }
            
            # 4. ì‹¤ì œ ì •ë¦¬ ìˆ˜í–‰
            cleaned_count = 0
            
            for analysis in cleanup_candidates:
                try:
                    self.minimize_comment(analysis.id, analysis.body, analysis.cleanup_reason)
                    cleaned_count += 1
                    time.sleep(0.5)  # API ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€
                except Exception as e:
                    print(f"âš ï¸ ëŒ“ê¸€ {analysis.id} ì •ë¦¬ ì‹¤íŒ¨: {e}")
            
            # 5. ì •ë¦¬ ìš”ì•½ ìƒì„±
            summary = self.create_cleanup_summary(issue_number, analyses, cleaned_count)
            
            result = {
                "success": True,
                "issue_number": issue_number,
                "total_comments": len(analyses),
                "comments_cleaned": cleaned_count,
                "comments_preserved": len(keep_candidates),
                "cleanup_summary_url": summary.get('html_url', ''),
                "cleanup_statistics": {
                    "spam_cleaned": len([a for a in cleanup_candidates if a.is_spam]),
                    "noise_cleaned": len([a for a in cleanup_candidates if a.is_noise]),
                    "duplicate_cleaned": len([a for a in cleanup_candidates if a.is_duplicate]),
                    "outdated_cleaned": len([a for a in cleanup_candidates if a.is_outdated])
                }
            }
            
            print(f"âœ… ì´ìŠˆ #{issue_number} ëŒ“ê¸€ ì •ë¦¬ ì™„ë£Œ")
            print(f"ğŸ“Š ê²°ê³¼: {cleaned_count}ê°œ ì •ë¦¬, {len(keep_candidates)}ê°œ ë³´ì¡´")
            
            return result
            
        except Exception as e:
            error_result = {
                "success": False,
                "issue_number": issue_number,
                "error": str(e)
            }
            
            print(f"âŒ ëŒ“ê¸€ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return error_result

def main():
    parser = argparse.ArgumentParser(description='SafeWork GitHub ì´ìŠˆ ëŒ“ê¸€ ì •ë¦¬ ì‹œìŠ¤í…œ')
    parser.add_argument('issue_number', type=int, help='ì •ë¦¬í•  ì´ìŠˆ ë²ˆí˜¸')
    parser.add_argument('--dry-run', action='store_true', 
                       help='ì‹¤ì œ ì •ë¦¬í•˜ì§€ ì•Šê³  ì‹œë®¬ë ˆì´ì…˜ë§Œ ì‹¤í–‰')
    parser.add_argument('--preserve-days', type=int, default=7,
                       help='ë³´í˜¸í•  ìµœê·¼ ëŒ“ê¸€ ì¼ìˆ˜ (ê¸°ë³¸: 7ì¼)')
    parser.add_argument('--min-importance', type=int, default=3,
                       help='ë³´ì¡´ ìµœì†Œ ì¤‘ìš”ë„ ì„ê³„ê°’ (ê¸°ë³¸: 3)')
    
    args = parser.parse_args()
    
    # í™˜ê²½ ë³€ìˆ˜ì—ì„œ GitHub ì„¤ì • ì½ê¸°
    github_token = os.environ.get('GITHUB_TOKEN')
    repository = os.environ.get('GITHUB_REPOSITORY')
    
    if not github_token:
        print("âŒ GITHUB_TOKEN í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        sys.exit(1)
        
    if not repository:
        print("âŒ GITHUB_REPOSITORY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        sys.exit(1)
    
    # ì •ë¦¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    cleanup_system = SafeWorkCommentCleanup(github_token, repository)
    
    # ì •ë¦¬ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
    result = cleanup_system.process_issue_cleanup(
        args.issue_number,
        dry_run=args.dry_run,
        preserve_recent_days=args.preserve_days,
        min_importance_threshold=args.min_importance
    )
    
    # ê²°ê³¼ ì¶œë ¥
    if result['success']:
        if result.get('dry_run'):
            print(f"ğŸ” ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼:")
            print(f"  - ì „ì²´ ëŒ“ê¸€: {result['total_comments']}ê°œ")
            print(f"  - ì •ë¦¬ ëŒ€ìƒ: {result['cleanup_candidates']}ê°œ")  
            print(f"  - ë³´ì¡´ ëŒ€ìƒ: {result['preservation_candidates']}ê°œ")
        else:
            print(f"ğŸ‰ ëŒ“ê¸€ ì •ë¦¬ ì™„ë£Œ!")
            print(f"ğŸ“Š ê²°ê³¼ ìš”ì•½:")
            print(f"  - ì´ìŠˆ ë²ˆí˜¸: #{result['issue_number']}")
            print(f"  - ì •ë¦¬ëœ ëŒ“ê¸€: {result['comments_cleaned']}ê°œ")
            print(f"  - ë³´ì¡´ëœ ëŒ“ê¸€: {result['comments_preserved']}ê°œ")
            if result.get('cleanup_summary_url'):
                print(f"  - ìš”ì•½ ëŒ“ê¸€: {result['cleanup_summary_url']}")
        sys.exit(0)
    else:
        print(f"âŒ ëŒ“ê¸€ ì •ë¦¬ ì‹¤íŒ¨: {result['error']}")
        sys.exit(1)

if __name__ == "__main__":
    main()